# -*- coding: utf-8 -*-
"""softmax_dlfc_label_me_complete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18j3DazMnnTIIAqv5tlAfEb1fF2c4ZThl
"""

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import os

from scipy.spatial import distance
from scipy.special import kl_div
from scipy.stats import entropy
from sklearn.metrics.pairwise import cosine_similarity
from tabulate import tabulate

from random import shuffle

from google.colab import drive

drive.mount('/content/drive')

"""**Configuration Parameters**"""

NUM_RUNS = 30
DATA_PATH = 'drive/My Drive/Data/LabelMe/prepared/'
N_CLASSES = 8
BATCH_SIZE = 64
N_EPOCHS = 50

"""**Loading the data**"""

def load_data(filename):
    f = open(filename, 'rb')
    data = np.load(f)
    f.close()
    return data

print("\nLoading train data...")

# originally just the train images but I need some of it to be used for soft label testing
train_test_vgg16 = load_data(DATA_PATH+"data_train_vgg16.npy")
train_test_labels = load_data(DATA_PATH+"labels_train.npy")

# indices = list(range(len(train_test_vgg16)))
# shuffle(indices)
# np.save(DATA_PATH+'shuffled_indices', np.array(indices))
indices = np.load(DATA_PATH+'shuffled_indices.npy').tolist()
train_indices = indices[1118:]
test_indices = indices[:1118]

data_train_vgg16 = train_test_vgg16[train_indices]
data_test_vgg16 = train_test_vgg16[test_indices]
labels_train = train_test_labels[train_indices]
labels_test = train_test_labels[test_indices]

print(data_train_vgg16.shape)
print(labels_train.shape)

# labels obtained from majority voting
labels_train_mv = load_data(DATA_PATH+"labels_train_mv.npy")[train_indices]
print(labels_train_mv.shape)
# labels obtained from Dawid and Skene aggregation
labels_train_ds = load_data(DATA_PATH+"labels_train_DS.npy")[train_indices]
print(labels_train_ds.shape)

# data from Amazon Mechanical Turk
print("\nLoading AMT data...")
all_answers = load_data(DATA_PATH+"answers.npy")

train_answers = all_answers[train_indices]
test_answers = all_answers[test_indices]

# dlfc data
filename1 = 'drive/My Drive/Data/LabelMe/train_anno_users.npy'
train_users = np.load(filename1)[train_indices]
filename2 = 'drive/My Drive/Data/LabelMe/train_anno_status.npy'
train_status =  np.load(filename2)[train_indices]

print(train_answers.shape, test_answers.shape)
N_ANNOT = train_answers.shape[1]
print("\nN_CLASSES:", N_CLASSES)
print("N_ANNOT:", N_ANNOT)


# load test data and validation data
print("\nLoading test and validation data...")

# test images and labels 
print(data_test_vgg16.shape)
print(labels_test.shape)

# validation images and labels
data_val_vgg16 = load_data(DATA_PATH+"data_valid_vgg16.npy")
print(data_val_vgg16.shape)
labels_val = load_data(DATA_PATH+"labels_valid.npy")
print(labels_val.shape)

"""**Converting Data to One-Hot Encoding**"""

def one_hot(target, n_classes):
    targets = np.array([target]).reshape(-1)
    one_hot_targets = np.eye(n_classes)[targets]
    return one_hot_targets

print("\nConverting to one-hot encoding...")
labels_train_bin = one_hot(labels_train, N_CLASSES)
print(labels_train_bin.shape)
labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)
print(labels_train_mv_bin.shape)
labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)
print(labels_train_ds_bin.shape)

labels_test_bin = one_hot(labels_test, N_CLASSES)
print(labels_test_bin.shape)
labels_val_bin = one_hot(labels_val, N_CLASSES)
print(labels_val_bin.shape)

"""**Getting the Soft Labels and the distributions**"""

from scipy.special import softmax

train_distrs = []
train_softs = []
train_entropys = []
norm = [1/N_CLASSES for i in range(N_CLASSES)]

for item in train_answers.tolist():
    distr = [item.count(i) for i in range(N_CLASSES)]
    train_distrs.append(distr)
    num_votes = sum(distr)
    soft = softmax(distr)
    train_softs.append(soft)
    ent = entropy(soft)/entropy(norm)
    train_entropys.append(ent)

test_distrs = []
test_softs = []
test_entropys = []

for item in test_answers.tolist():
    distr = [item.count(i) for i in range(N_CLASSES)]
    test_distrs.append(distr)
    num_votes = sum(distr)
    soft = softmax(distr)
    test_softs.append(soft)
    ent = entropy(soft)/entropy(norm)
    test_entropys.append(ent)

len(train_softs), len(test_softs)

"""**Evaluation Metrics**"""

def get_acc_f1(test_trues, test_preds):
    total = 0
    correct = 0

    matches = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    gold = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    system = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}

    for p, g in zip(test_preds,test_trues):
        total+=1
        if p == g:
            correct+=1
            matches[p] += 1

        gold[g] += 1
        system[p] += 1
    
    
    recall = {}
    precision = {}
    f1 = {}
    for i in range(N_CLASSES):
        recall[i] = 1.0 * matches[i] / gold[i] if matches[i] != 0 else 0
        precision[i] = 1.0 * matches[i] / system[i] if matches[i] !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0

    support = np.array([gold[i] for i in range(N_CLASSES)])

    average_recall = np.average([recall[i] for i in range(N_CLASSES)], weights=support)
    average_recall = np.average([recall[i] for i in range(N_CLASSES)], weights=support)
    average_precision = np.average([precision[i] for i in range(N_CLASSES)], weights=support)
    average_f1 = np.average([f1[i] for i in range(N_CLASSES)], weights=support)
    
    acc = correct/total

    return acc, average_precision, average_recall, average_f1

def get_ct_f1(test_trues, test_preds, test_distrs, num_classes=N_CLASSES):
    tp = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    fp = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    fn = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}

    gold = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    
    
    for p, g, distr in zip(test_preds, test_trues, test_distrs):
        # srs score of the gold not the predicted. Todo: talk it over with Massimo
        unit_vector = np.array([1 if i==g else 0 for i in range(num_classes)]).reshape(1,num_classes)
        distr = np.array(distr).reshape(1, num_classes)
        srs_s = cosine_similarity(unit_vector, distr)[0][0]
            
        if p == g:
            tp[p] += srs_s # correct hit
        else:
            fp[p] += (1-srs_s)  # miss
            fn[g] += srs_s   # correct rejection
    
        gold[g] += 1

    recall = {}
    precision = {}
    f1 = {}
    for i in range(num_classes):
        precision[i] = 1.0 * tp[i] / (tp[i] + fp[i]) if (fp[i] + tp[i]) != 0 else 0
        recall[i] = 1.0 * tp[i] / (tp[i] + fn[i]) if (fn[i] + tp[i]) !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0
        
    support = np.array([gold[i] for i in range(num_classes)])

    average_recall = np.average([recall[i] for i in range(num_classes)], weights=support)
    average_precision = np.average([precision[i] for i in range(num_classes)], weights=support)
    average_f1 = np.average([f1[i] for i in range(num_classes)], weights=support)

        
    return average_precision, average_recall, average_f1

def get_jsd_kl_div(soft_probs, predicted_probs):
    num_items = len(predicted_probs)
    all_jsd = [distance.jensenshannon(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    all_kl = [kl_div(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    return np.sum(all_jsd)/num_items, np.sum(all_kl)/num_items

def cross_entropy(predictions, targets, epsilon=1e-12):
    """
    Computes cross entropy between targets (encoded as one-hot vectors)
    and predictions.
    Input: predictions (N, k) ndarray
           targets (N, k) ndarray
    Returns: scalar
    """
    predictions = np.clip(predictions, epsilon, 1. - epsilon)
    N = predictions.shape[0]
    ce = -np.sum(targets*np.log(predictions))/N
    return ce

"""**Defining the Deep Learning Model**

Here we shall use features representation produced by the VGG16 network as the input. Our base model is then simply composed by one densely-connected layer with 128 hidden units and an output dense layer. We use 50% dropout between the two dense layers.
"""

num_users = 59
hotsize = 8
def to_cuda(x):
    """ GPU-enable a tensor """
    if torch.cuda.is_available():
        x = x.cuda()
    return x

class LabelMe_Classifier(nn.Module):
    def __init__(self):
        super().__init__()
        
        self.hidden_layer = nn.Sequential(nn.Linear(8192, 128), 
                                          nn.LeakyReLU(),
                                          nn.Dropout(0.2))
        self.output_layer = nn.Sequential(nn.Linear(128, N_CLASSES))
        beta = torch.eye(hotsize)
        self.global_user_confusion = torch.nn.Parameter(beta.unsqueeze(0).repeat([num_users, 1, 1]))

    def forward(self, data_input, anno_users, anno_status, test_mode=False):
        data_input = torch.flatten(data_input, 1)
        h = self.hidden_layer(data_input)
        predictions = self.output_layer(h)
        softmax_scores = torch.softmax(predictions, 1)
        if test_mode:
            return None, softmax_scores
        else:
            batch_size = anno_users.shape[0]
            max_anno_length = anno_users.shape[1]

            # flatten the user and class information
            flattened_anno_users = torch.flatten(anno_users) # [num_mentions, max_anno_length]
            flattened_anno_status = torch.flatten(anno_status)  # [num_mentions, max_anno_length]

            # use the mask to remove the useless padding-injected information from the user and the class tensors; one hot encode the latter
            valid_users = flattened_anno_users[flattened_anno_users>-1]         # [across_mention_annotations]
            valid_judgements = flattened_anno_status[flattened_anno_status>-1]     # [across_mention_annotations]
            judgements_one_hot = to_cuda(torch.eye(hotsize)[valid_judgements])     # [across_mention_annotations, 4]

            # extract the confusion matrices for the users who provided the annotations
            users_confusions = torch.index_select(self.global_user_confusion, 0, valid_users)  # [across_mention_annotations, 4, 4]

            # the scores produced by the neural network for each mention are used in the computation of every expected annotator response; so duplicate them appropriately
            duplicated_scores = predictions.unsqueeze(1)   # [num_mentions, 1, 4]
            duplicated_scores = duplicated_scores.repeat(1, max_anno_length, 1)    # [num_mentions, max_anno_length, 4]

            duplicated_scores = torch.reshape(duplicated_scores, (batch_size*max_anno_length, hotsize))  # [num_mentions * max_anno_length, 4]
                       
            # corresponding mention scores for each annotation provided
            selected_scores = duplicated_scores[flattened_anno_status>-1]    # [across_mention_annotations, 4]
            # the expected annotator respone is a linear combination of the annotator confusion and the neural network prediction
            expected_user_responses = torch.matmul(users_confusions, selected_scores.unsqueeze(2)).squeeze(2)  # [across_mention_annotations, 4]

            labels = torch.argmax(judgements_one_hot,1)
            loss = torch.nn.functional.nll_loss(torch.nn.functional.log_softmax(expected_user_responses, 1), labels)

            return loss, softmax_scores

"""**Training using the Gold Labels**"""

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

dlc_dictionary = {}

val_labels = labels_val.tolist()
test_labels = labels_test.tolist()
NUM_TEST = len(test_labels)

for exp in range(NUM_EXPERIMENTS):
    print('\nExperiment %d ###########'%exp)

    train_x, test_x, val_x = torch.tensor(data_train_vgg16).float().cuda(), torch.tensor(data_test_vgg16).float().cuda(), torch.tensor(data_val_vgg16).float().cuda()
    train_y1, train_y2 = torch.tensor(train_users).long().cuda(), torch.tensor(train_status).long().cuda()
    test_y1, test_y2 = torch.tensor(labels_test_bin).float().cuda(), torch.tensor(test_softs).float().cuda()
    val_y1 = torch.tensor(labels_val_bin).float().cuda()

    # load embeddings for that dataset
    model = LabelMe_Classifier().cuda()
    optimizer = torch.optim.Adam(model.parameters())

    best_val_f = 0.0

    for epoch in range(1, N_EPOCHS+1):

        model.train()
        loss, _ = model(train_x, train_y1, train_y2, False)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        model.eval()
        _, val_pred = model(val_x, None, None, True)
        val_acc, val_p, val_r, val_f = get_acc_f1(val_labels, torch.argmax(val_pred, 1).detach().cpu().numpy().tolist())
        if val_f > best_val_f:
            best_val_f = val_f
            torch.save(model.state_dict(), DATA_PATH+'best_model.pt')  

    model = LabelMe_Classifier().cuda()
    model.load_state_dict(torch.load(DATA_PATH+'best_model.pt'))
    model.eval()
    _, test_pred = model(test_x, None, None, True)
    test_pred = test_pred.detach().cpu().numpy()
    predicted_test_labels = np.argmax(test_pred, 1).tolist()

    dlc_dictionary[str(exp)] = test_pred.tolist()

    test_acc, test_p, test_r, test_f = get_acc_f1(test_labels, predicted_test_labels)
    cp, cr, cf = get_ct_f1(test_labels, predicted_test_labels, test_distrs)

    jsd, kl = get_jsd_kl_div(test_softs, test_pred)

    preds_ents = [entropy(p)/entropy(norm) for p in test_pred]

    ent = cosine_similarity(np.array(test_entropys).reshape(1, NUM_TEST), np.array(preds_ents).reshape(1, NUM_TEST))[0][0]

    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_pred, test_softs)
    
    accs.append(test_acc)
    prfs.append([test_p, test_r, test_f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(test_acc, test_f, cf, jsd, kl, ent, ce_res)
    print('#'*50)

import json

writepath = writepath = 'drive/My Drive/Colab Notebooks/Significance_Testing/lm_experiments/'

name = 'dlc'
with open(writepath+'lm_' + name + '.jsonlines', 'w') as f:
    json.dump(dlc_dictionary, f)


print('DLFC Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nDLFC PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nDLFC Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nDLFC JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nDLFC KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nDLFC entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nDLFC entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nDLFC cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

