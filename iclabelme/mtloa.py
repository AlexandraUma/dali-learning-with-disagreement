# -*- coding: utf-8 -*-
"""softmax_mtl_uma_labelme.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PIWMoNngjR8Ym8KGf-FXQL_mYwXmzDih
"""

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import os

from scipy.spatial import distance
from scipy.special import kl_div
from scipy.stats import entropy
from sklearn.metrics.pairwise import cosine_similarity
from tabulate import tabulate

from random import shuffle

from google.colab import drive

drive.mount('/content/drive')

"""**Configuration Parameters**"""

NUM_RUNS = 30
DATA_PATH = 'drive/My Drive/Data/LabelMe/prepared/'
N_CLASSES = 8
BATCH_SIZE = 64
N_EPOCHS = 50

"""**Loading the data**"""

def load_data(filename):
    f = open(filename, 'rb')
    data = np.load(f)
    f.close()
    return data

print("\nLoading train data...")

# originally just the train images but I need some of it to be used for soft label testing
train_test_vgg16 = load_data(DATA_PATH+"data_train_vgg16.npy")
train_test_labels = load_data(DATA_PATH+"labels_train.npy")

indices = np.load(DATA_PATH+'shuffled_indices.npy').tolist()
train_indices = indices[1118:]
test_indices = indices[:1118]

data_train_vgg16 = train_test_vgg16[train_indices]
data_test_vgg16 = train_test_vgg16[test_indices]
labels_train = train_test_labels[train_indices]
labels_test = train_test_labels[test_indices]

print(data_train_vgg16.shape)
print(labels_train.shape)

# labels obtained from majority voting
labels_train_mv = load_data(DATA_PATH+"labels_train_mv.npy")[train_indices]
print(labels_train_mv.shape)
# labels obtained from Dawid and Skene aggregation
labels_train_ds = load_data(DATA_PATH+"labels_train_DS.npy")[train_indices]
print(labels_train_ds.shape)

# data from Amazon Mechanical Turk
print("\nLoading AMT data...")
all_answers = load_data(DATA_PATH+"answers.npy")

train_answers = all_answers[train_indices]
test_answers = all_answers[test_indices]

print(train_answers.shape, test_answers.shape)
N_ANNOT = train_answers.shape[1]
print("\nN_CLASSES:", N_CLASSES)
print("N_ANNOT:", N_ANNOT)


# load test data and validation data
print("\nLoading test and validation data...")

# test images and labels 
print(data_test_vgg16.shape)
print(labels_test.shape)

# validation images and labels
data_val_vgg16 = load_data(DATA_PATH+"data_valid_vgg16.npy")
print(data_val_vgg16.shape)
labels_val = load_data(DATA_PATH+"labels_valid.npy")
print(labels_val.shape)

"""**Converting Data to One-Hot Encoding**"""

def one_hot(target, n_classes):
    targets = np.array([target]).reshape(-1)
    one_hot_targets = np.eye(n_classes)[targets]
    return one_hot_targets

print("\nConverting to one-hot encoding...")
labels_train_bin = one_hot(labels_train, N_CLASSES)
print(labels_train_bin.shape)
labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)
print(labels_train_mv_bin.shape)
labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)
print(labels_train_ds_bin.shape)

labels_test_bin = one_hot(labels_test, N_CLASSES)
print(labels_test_bin.shape)
labels_val_bin = one_hot(labels_val, N_CLASSES)
print(labels_val_bin.shape)

from scipy.special import softmax

train_distrs = []
train_softs = []
train_entropys = []
norm = [1/N_CLASSES for i in range(N_CLASSES)]

for item in train_answers.tolist():
    distr = [item.count(i) for i in range(N_CLASSES)]
    train_distrs.append(distr)
    num_votes = sum(distr)
    soft = softmax(distr)
    train_softs.append(soft)
    ent = entropy(soft)/entropy(norm)
    train_entropys.append(ent)

test_distrs = []
test_softs = []
test_entropys = []

for item in test_answers.tolist():
    distr = [item.count(i) for i in range(N_CLASSES)]
    test_distrs.append(distr)
    num_votes = sum(distr)
    soft = softmax(distr)
    test_softs.append(soft)
    ent = entropy(soft)/entropy(norm)
    test_entropys.append(ent)

len(train_softs), len(test_softs)

"""**Weighting By Observed Agreement**"""

def observed_agreement(annotations):
    """Aggrement is computed using Observed Agreement instead of
    Kappa i.e. (Ao-Ae)/(1-Ae) as expected agreement is not computed
    on a per item basis. TODO confirm with Massimo.
    """
    c = len (annotations)
    niks = [annotations.count(i) for i in set(annotations)]
    # getting the summ product
    numerator = sum([i*(i-1) for i in niks])
    if c == 1:
        return 1.0  # if only one annotator annotated it, it is a perfect agreement
    return numerator/(c*(c-1))

valid_annotations = [[i for i in ans if i != -1] for ans in train_answers]
item_weights = np.array([observed_agreement(a) for a in valid_annotations])

valid_test = [[i for i in ans if i != -1] for ans in test_answers]
test_diffs = np.array([observed_agreement(a) for a in valid_test])

"""**Evaluation Metrics**"""

def get_acc_f1(test_trues, test_preds):
    total = 0
    correct = 0

    matches = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    gold = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    system = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}

    for p, g in zip(test_preds,test_trues):
        total+=1
        if p == g:
            correct+=1
            matches[p] += 1

        gold[g] += 1
        system[p] += 1
    
    
    recall = {}
    precision = {}
    f1 = {}
    for i in range(N_CLASSES):
        recall[i] = 1.0 * matches[i] / gold[i] if matches[i] != 0 else 0
        precision[i] = 1.0 * matches[i] / system[i] if matches[i] !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0

    support = np.array([gold[i] for i in range(N_CLASSES)])

    average_recall = np.average([recall[i] for i in range(N_CLASSES)], weights=support)
    average_recall = np.average([recall[i] for i in range(N_CLASSES)], weights=support)
    average_precision = np.average([precision[i] for i in range(N_CLASSES)], weights=support)
    average_f1 = np.average([f1[i] for i in range(N_CLASSES)], weights=support)
    
    acc = correct/total

    return acc, average_precision, average_recall, average_f1

def get_ct_f1(test_trues, test_preds, test_distrs, num_classes=N_CLASSES):
    tp = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    fp = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    fn = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}

    gold = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    
    
    for p, g, distr in zip(test_preds, test_trues, test_distrs):
        # srs score of the gold not the predicted. Todo: talk it over with Massimo
        unit_vector = np.array([1 if i==g else 0 for i in range(num_classes)]).reshape(1,num_classes)
        distr = np.array(distr).reshape(1, num_classes)
        srs_s = cosine_similarity(unit_vector, distr)[0][0]
            
        if p == g:
            tp[p] += srs_s # correct hit
        else:
            fp[p] += (1-srs_s)  # miss
            fn[g] += srs_s   # correct rejection
    
        gold[g] += 1

    recall = {}
    precision = {}
    f1 = {}
    for i in range(num_classes):
        precision[i] = 1.0 * tp[i] / (tp[i] + fp[i]) if (fp[i] + tp[i]) != 0 else 0
        recall[i] = 1.0 * tp[i] / (tp[i] + fn[i]) if (fn[i] + tp[i]) !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0
        
    support = np.array([gold[i] for i in range(num_classes)])

    average_recall = np.average([recall[i] for i in range(num_classes)], weights=support)
    average_precision = np.average([precision[i] for i in range(num_classes)], weights=support)
    average_f1 = np.average([f1[i] for i in range(num_classes)], weights=support)

        
    return average_precision, average_recall, average_f1

def get_jsd_kl_div(soft_probs, predicted_probs):
    num_items = len(predicted_probs)
    all_jsd = [distance.jensenshannon(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    all_kl = [kl_div(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    return np.sum(all_jsd)/num_items, np.sum(all_kl)/num_items

def cross_entropy(predictions, targets, epsilon=1e-12):
    """
    Computes cross entropy between targets (encoded as one-hot vectors)
    and predictions.
    Input: predictions (N, k) ndarray
           targets (N, k) ndarray
    Returns: scalar
    """
    predictions = np.clip(predictions, epsilon, 1. - epsilon)
    N = predictions.shape[0]
    ce = -np.sum(targets*np.log(predictions))/N
    return ce

"""**Defining the Deep Learning Model**

Here we shall use features representation produced by the VGG16 network as the input. Our base model is then simply composed by one densely-connected layer with 128 hidden units and an output dense layer. We use 50% dropout between the two dense layers.
"""

class LabelMe_Classifier(nn.Module):
    def __init__(self, model_type='stl', smythe=None):
        super().__init__()
        
        self.hidden_layer = nn.Sequential(nn.Linear(8192, 128), 
                                          nn.ReLU(),
                                          nn.Dropout(0.5))
        self.output_layer = nn.Sequential(nn.Linear(128, N_CLASSES))
        self.output_diff = nn.Sequential(nn.Linear(128, 1))


    def forward(self, data_input, one_hot_labels, soft_labels, weights, eval=False):
        data_input = torch.flatten(data_input, 1)
        h = self.hidden_layer(data_input)
        output = self.output_layer(h)
        softmax_scores = torch.softmax(output, 1)

        # output of the auxilary task
        pred_diff = self.output_diff(h)
        item_diff = torch.sigmoid(pred_diff).squeeze(1)
        if eval:
            return softmax_scores, item_diff, 0.0, 0.0
        else:
            cross_entropy = torch.sum(torch.mul(one_hot_labels, softmax_scores.log()), 1) 
            main_loss = -torch.sum(cross_entropy)

            aux_loss = torch.nn.MSELoss(reduction='sum')(weights, item_diff)
            return softmax_scores, item_diff, main_loss, aux_loss

"""**Training using the Gold Labels**"""

device = 'cuda' if torch.cuda.is_available() else 'cpu'

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

similarity_diffs = []
diffs_correlation = []

mtl_uma_dictionary = {}

val_labels = labels_val.tolist()
test_labels = labels_test.tolist()
NUM_TEST = len(test_labels)

for exp in range(NUM_EXPERIMENTS):
    print('\nExperiment %d ###########'%exp)

    train_x, test_x, val_x = torch.tensor(data_train_vgg16).float().to(device), torch.tensor(data_test_vgg16).float().to(device), torch.tensor(data_val_vgg16).float().to(device)
    train_y1, train_y2 = torch.tensor(labels_train_bin).float().to(device), torch.tensor(train_softs).float().to(device)
    test_y1, test_y2 = torch.tensor(labels_test_bin).float().to(device), torch.tensor(test_softs).float().to(device)
    val_y1 = torch.tensor(labels_val_bin).float().to(device)
    train_weights = torch.tensor(item_weights).float().to(device)

    # load embeddings for that dataset
    model = LabelMe_Classifier('mtl').to(device)
    optimizer = torch.optim.Adam(model.parameters())

    best_val_f = 0.0

    for epoch in range(1, N_EPOCHS+1):

        model.train()
        _,_ , hard_loss, aux_loss = model(train_x, train_y1, train_y2, train_weights, False)
        optimizer.zero_grad()
        aux_loss.backward(retain_graph=True)
        hard_loss.backward()
        optimizer.step()

        model.eval()
        val_pred, _, _, _ = model(val_x, val_y1, None, None, True)
        val_acc, val_p, val_r, val_f = get_acc_f1(val_labels, torch.argmax(val_pred, 1).detach().cpu().numpy().tolist())
        if val_f > best_val_f:
            best_val_f = val_f
            torch.save(model.state_dict(), DATA_PATH+'best_model.pt')  

    model = LabelMe_Classifier('mtl').to(device)
    model.load_state_dict(torch.load(DATA_PATH+'best_model.pt'))
    model.eval()
    test_pred, test_pred_diffs, _, _ = model(test_x, test_y1, test_y2, None, True)
    test_pred = test_pred.detach().cpu().numpy()
    predicted_test_labels = np.argmax(test_pred, 1).tolist()

    mtl_uma_dictionary[str(exp)] = test_pred.tolist()

    test_acc, test_p, test_r, test_f = get_acc_f1(test_labels, predicted_test_labels)
    cp, cr, cf = get_ct_f1(test_labels, predicted_test_labels, test_distrs)

    jsd, kl = get_jsd_kl_div(test_softs, test_pred)

    preds_ents = [entropy(p)/entropy(norm) for p in test_pred]

    ent = cosine_similarity(np.array(test_entropys).reshape(1, NUM_TEST), np.array(preds_ents).reshape(1, NUM_TEST))[0][0]

    corr = np.corrcoef(test_entropys, preds_ents)[0][1]

    ce_res = cross_entropy(test_pred, test_softs)

    itdiff = cosine_similarity(np.array(test_diffs).reshape(1, NUM_TEST), np.array(test_pred_diffs.detach().cpu().numpy()).reshape(1, NUM_TEST))[0][0]
    itcorr = np.corrcoef(test_diffs, test_pred_diffs.detach().cpu().numpy())[0][1]
    

    accs.append(test_acc)
    prfs.append([test_p, test_r, test_f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(test_acc, test_f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

import json

writepath = writepath = 'drive/My Drive/Colab Notebooks/Significance_Testing/lm_experiments/'

name = 'mtl_oa'
with open(writepath+'lm_' + name + '.jsonlines', 'w') as f:
    json.dump(mtl_uma_dictionary, f)


print('MTL Accuracy stats after 30 experiments: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nMTL PRF stats after 30 experiments:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nMTL Crowdtruth PRF stats after 30 experiments:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nMTL JSD stats after 30 experiments: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nMTL KL stats after 30 experiments: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nMTL entropy similarity stats after 30 experiments: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nMTL entropy correlation stats after 30 experiments: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nMTL cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

"""**USING MV**"""

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []

similarity_diffs = []
diffs_correlation = []

mv_mtloa_dictionary = {}

val_labels = labels_val.tolist()
test_labels = labels_test.tolist()
NUM_TEST = len(test_labels)

for exp in range(NUM_EXPERIMENTS):
    print('\nExperiment %d ###########'%exp)

    train_x, test_x, val_x = torch.tensor(data_train_vgg16).float().cuda(), torch.tensor(data_test_vgg16).float().cuda(), torch.tensor(data_val_vgg16).float().cuda()
    train_y1, train_y2 = torch.tensor(labels_train_mv_bin).float().cuda(), torch.tensor(train_softs).float().cuda()
    test_y1, test_y2 = torch.tensor(labels_test_bin).float().cuda(), torch.tensor(test_softs).float().cuda()
    val_y1 = torch.tensor(labels_val_bin).float().cuda()
    train_weights = torch.tensor(item_weights).float().cuda()

    # load embeddings for that dataset
    model = LabelMe_Classifier('stl').cuda()
    optimizer = torch.optim.Adam(model.parameters())

    best_val_f = 0.0

    for epoch in range(1, N_EPOCHS+1):

        model.train()
        _,_ , hard_loss, aux_loss = model(train_x, train_y1, train_y2, train_weights, False)
        optimizer.zero_grad()
        aux_loss.backward(retain_graph=True)
        hard_loss.backward()
        optimizer.step()

        model.eval()
        val_pred, _, _, _ = model(val_x, val_y1, None, None, True)
        val_acc, val_p, val_r, val_f = get_acc_f1(val_labels, torch.argmax(val_pred, 1).detach().cpu().numpy().tolist())
        if val_f > best_val_f:
            best_val_f = val_f
            torch.save(model.state_dict(), DATA_PATH+'best_model.pt')  

    model = LabelMe_Classifier('stl').cuda()
    model.load_state_dict(torch.load(DATA_PATH+'best_model.pt'))
    model.eval()
    test_pred, test_pred_diffs, _, _ = model(test_x, test_y1, test_y2, None, True)
    test_pred = test_pred.detach().cpu().numpy()
    predicted_test_labels = np.argmax(test_pred, 1).tolist()

    mv_mtloa_dictionary[str(exp)] = test_pred.tolist()

    test_acc, test_p, test_r, test_f = get_acc_f1(test_labels, predicted_test_labels)
    cp, cr, cf = get_ct_f1(test_labels, predicted_test_labels, test_distrs)

    jsd, kl = get_jsd_kl_div(test_softs, test_pred)

    preds_ents = [entropy(p)/entropy(norm) for p in test_pred]

    ent = cosine_similarity(np.array(test_entropys).reshape(1, NUM_TEST), np.array(preds_ents).reshape(1, NUM_TEST))[0][0]

    corr = np.corrcoef(test_entropys, preds_ents)[0][1]

    itdiff = cosine_similarity(np.array(test_diffs).reshape(1, NUM_TEST), np.array(test_pred_diffs.detach().cpu().numpy()).reshape(1, NUM_TEST))[0][0]
    itcorr = np.corrcoef(test_diffs, test_pred_diffs.detach().cpu().numpy())[0][1]
    
    accs.append(test_acc)
    prfs.append([test_p, test_r, test_f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    
    print(test_acc, test_f, cf, jsd, kl, ent, corr)
    print('#'*50)

import json

writepath = writepath = 'drive/My Drive/Colab Notebooks/Significance_Testing/lm_experiments/'

name = 'mv_mtl_oa'
with open(writepath+'lm_' + name + '.jsonlines', 'w') as f:
    json.dump(mv_mtloa_dictionary, f)


print('MTL Accuracy stats after 30 experiments: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nMTL PRF stats after 30 experiments:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nMTL Crowdtruth PRF stats after 30 experiments:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nMTL JSD stats after 30 experiments: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nMTL KL stats after 30 experiments: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nMTL entropy similarity stats after 30 experiments: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nMTL entropy correlation stats after 30 experiments: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

