# -*- coding: utf-8 -*-
"""softmax_ct_label_me.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SiWD8JlmrqAmmTQbqlHaiHXnqoEWyYkF
"""

import numpy as np

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

import os

from scipy.spatial import distance
from scipy.special import kl_div
from scipy.stats import entropy
from sklearn.metrics.pairwise import cosine_similarity
from tabulate import tabulate

from random import shuffle

from google.colab import drive

drive.mount('/content/drive')

"""**Configuration Parameters**"""

NUM_RUNS = 30
DATA_PATH = 'drive/My Drive/Data/LabelMe/prepared/'
N_CLASSES = 8
BATCH_SIZE = 64
N_EPOCHS = 50

"""**Loading the data**"""

def load_data(filename):
    f = open(filename, 'rb')
    data = np.load(f)
    f.close()
    return data

print("\nLoading train data...")

# originally just the train images but I need some of it to be used for soft label testing
train_test_vgg16 = load_data(DATA_PATH+"data_train_vgg16.npy")
train_test_labels = load_data(DATA_PATH+"labels_train.npy")

# indices = list(range(len(train_test_vgg16)))
# shuffle(indices)
# np.save(DATA_PATH+'shuffled_indices', np.array(indices))
indices = np.load(DATA_PATH+'shuffled_indices.npy').tolist()
train_indices = indices[1118:]
test_indices = indices[:1118]

data_train_vgg16 = train_test_vgg16[train_indices]
data_test_vgg16 = train_test_vgg16[test_indices]
labels_train = train_test_labels[train_indices]
labels_test = train_test_labels[test_indices]

print(data_train_vgg16.shape)
print(labels_train.shape)

# labels obtained from majority voting
labels_train_mv = load_data(DATA_PATH+"labels_train_mv.npy")[train_indices]
print(labels_train_mv.shape)
# labels obtained from Dawid and Skene aggregation
labels_train_ds = load_data(DATA_PATH+"labels_train_DS.npy")[train_indices]
print(labels_train_ds.shape)

# data from Amazon Mechanical Turk
print("\nLoading AMT data...")
all_answers = load_data(DATA_PATH+"answers.npy")

train_answers = all_answers[train_indices]
test_answers = all_answers[test_indices]

print(train_answers.shape, test_answers.shape)
N_ANNOT = train_answers.shape[1]
print("\nN_CLASSES:", N_CLASSES)
print("N_ANNOT:", N_ANNOT)


# load test data and validation data
print("\nLoading test and validation data...")

# test images and labels 
print(data_test_vgg16.shape)
print(labels_test.shape)

# validation images and labels
data_val_vgg16 = load_data(DATA_PATH+"data_valid_vgg16.npy")
print(data_val_vgg16.shape)
labels_val = load_data(DATA_PATH+"labels_valid.npy")
print(labels_val.shape)

"""**Converting Data to One-Hot Encoding**"""

def one_hot(target, n_classes):
    targets = np.array([target]).reshape(-1)
    one_hot_targets = np.eye(n_classes)[targets]
    return one_hot_targets

print("\nConverting to one-hot encoding...")
labels_train_bin = one_hot(labels_train, N_CLASSES)
print(labels_train_bin.shape)
labels_train_mv_bin = one_hot(labels_train_mv, N_CLASSES)
print(labels_train_mv_bin.shape)
labels_train_ds_bin = one_hot(labels_train_ds, N_CLASSES)
print(labels_train_ds_bin.shape)

labels_test_bin = one_hot(labels_test, N_CLASSES)
print(labels_test_bin.shape)
labels_val_bin = one_hot(labels_val, N_CLASSES)
print(labels_val_bin.shape)

"""**Getting the Soft Labels and the distributions**"""

from scipy.special import softmax

train_distrs = []
train_softs = []
train_entropys = []
norm = [1/N_CLASSES for i in range(N_CLASSES)]

for item in train_answers.tolist():
    distr = [item.count(i) for i in range(N_CLASSES)]
    train_distrs.append(distr)
    num_votes = sum(distr)
    soft = softmax(distr)
    train_softs.append(soft)
    ent = entropy(soft)/entropy(norm)
    train_entropys.append(ent)

test_distrs = []
test_softs = []
test_entropys = []

for item in test_answers.tolist():
    distr = [item.count(i) for i in range(N_CLASSES)]
    test_distrs.append(distr)
    num_votes = sum(distr)
    soft = softmax(distr)
    test_softs.append(soft)
    ent = entropy(soft)/entropy(norm)
    test_entropys.append(ent)

len(train_softs), len(test_softs)

"""**Getting CrowdTruth SRS vector for training**"""

train_srs = []
for item in train_distrs:
    item = np.array(item).reshape(1, N_CLASSES)
    srs_vector = []
    for label in range(N_CLASSES):
        unit_vector = np.array([0] * N_CLASSES).reshape(1, N_CLASSES)
        unit_vector[0][label] = 1
        srs = cosine_similarity(item, unit_vector)
        srs_vector.append(srs[0][0])
    # print(item, srs_vector)
    train_srs.append(srs_vector)

train_distrs[4:8]

train_srs[4:8]

"""**Evaluation Metrics**"""

def get_acc_f1(test_trues, test_preds):
    total = 0
    correct = 0

    matches = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    gold = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    system = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}

    for p, g in zip(test_preds,test_trues):
        total+=1
        if p == g:
            correct+=1
            matches[p] += 1

        gold[g] += 1
        system[p] += 1
    
    
    recall = {}
    precision = {}
    f1 = {}
    for i in range(N_CLASSES):
        recall[i] = 1.0 * matches[i] / gold[i] if matches[i] != 0 else 0
        precision[i] = 1.0 * matches[i] / system[i] if matches[i] !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0

    support = np.array([gold[i] for i in range(N_CLASSES)])

    average_recall = np.average([recall[i] for i in range(N_CLASSES)], weights=support)
    average_recall = np.average([recall[i] for i in range(N_CLASSES)], weights=support)
    average_precision = np.average([precision[i] for i in range(N_CLASSES)], weights=support)
    average_f1 = np.average([f1[i] for i in range(N_CLASSES)], weights=support)
    
    acc = correct/total

    return acc, average_precision, average_recall, average_f1

def get_ct_f1(test_trues, test_preds, test_distrs, num_classes=N_CLASSES):
    tp = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    fp = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    fn = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}

    gold = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0}
    
    
    for p, g, distr in zip(test_preds, test_trues, test_distrs):
        # srs score of the gold not the predicted. Todo: talk it over with Massimo
        unit_vector = np.array([1 if i==g else 0 for i in range(num_classes)]).reshape(1,num_classes)
        distr = np.array(distr).reshape(1, num_classes)
        srs_s = cosine_similarity(unit_vector, distr)[0][0]
            
        if p == g:
            tp[p] += srs_s # correct hit
        else:
            fp[p] += (1-srs_s)  # miss
            fn[g] += srs_s   # correct rejection
    
        gold[g] += 1

    recall = {}
    precision = {}
    f1 = {}
    for i in range(num_classes):
        precision[i] = 1.0 * tp[i] / (tp[i] + fp[i]) if (fp[i] + tp[i]) != 0 else 0
        recall[i] = 1.0 * tp[i] / (tp[i] + fn[i]) if (fn[i] + tp[i]) !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0
        
    support = np.array([gold[i] for i in range(num_classes)])

    average_recall = np.average([recall[i] for i in range(num_classes)], weights=support)
    average_precision = np.average([precision[i] for i in range(num_classes)], weights=support)
    average_f1 = np.average([f1[i] for i in range(num_classes)], weights=support)

        
    return average_precision, average_recall, average_f1

def get_jsd_kl_div(soft_probs, predicted_probs):
    num_items = len(predicted_probs)
    all_jsd = [distance.jensenshannon(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    all_kl = [kl_div(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    return np.sum(all_jsd)/num_items, np.sum(all_kl)/num_items

def cross_entropy(predictions, targets, epsilon=1e-12):
    """
    Computes cross entropy between targets (encoded as one-hot vectors)
    and predictions.
    Input: predictions (N, k) ndarray
           targets (N, k) ndarray
    Returns: scalar
    """
    predictions = np.clip(predictions, epsilon, 1. - epsilon)
    N = predictions.shape[0]
    ce = -np.sum(targets*np.log(predictions))/N
    return ce

"""**Defining the Deep Learning Model**

Here we shall use features representation produced by the VGG16 network as the input. Our base model is then simply composed by one densely-connected layer with 128 hidden units and an output dense layer. We use 50% dropout between the two dense layers.
"""

class LabelMe_Classifier(nn.Module):
    def __init__(self, model_type='stl', smythe=None):
        super().__init__()
        
        self.hidden_layer = nn.Sequential(nn.Linear(8192, 128), 
                                          nn.LeakyReLU(),
                                          nn.Dropout(0.2))
        self.output_layer = nn.Sequential(nn.Linear(128, N_CLASSES))


    def forward(self, data_input, one_hot_labels, soft_labels, eval=False):
        data_input = torch.flatten(data_input, 1)
        h = self.hidden_layer(data_input)
        output = self.output_layer(h)
        # the output layer is a sigmoid so that each item has a chance between 0 and 1
        softmax_output = torch.softmax(output, 1)
        if eval:
            return _, softmax_output
        else:
            loss = torch.nn.MSELoss(reduction='sum')(soft_labels, softmax_output)
            return loss, softmax_output

"""**Training using the SRS scores but softmaxing output of model**"""

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

ct_dictionary = {}
true_labels_hard = {}

val_labels = labels_val.tolist()
test_labels = labels_test.tolist()
NUM_TEST = len(test_labels)

for exp in range(NUM_EXPERIMENTS):
    print('\nExperiment %d ###########'%exp)

    train_x, test_x, val_x = torch.tensor(data_train_vgg16).float().cuda(), torch.tensor(data_test_vgg16).float().cuda(), torch.tensor(data_val_vgg16).float().cuda()
    train_y1, train_y2 = torch.tensor(labels_train_bin).float().cuda(), torch.tensor(train_srs).float().cuda()
    test_y1, test_y2 = torch.tensor(labels_test_bin).float().cuda(), torch.tensor(test_softs).float().cuda()
    val_y1 = torch.tensor(labels_val_bin).float().cuda()

    # load embeddings for that dataset
    model = LabelMe_Classifier('stl').cuda()
    optimizer = torch.optim.Adam(model.parameters())

    best_val_f = 0.0

    for epoch in range(1, N_EPOCHS+1):

        model.train()
        loss, _ = model(train_x, train_y1, train_y2)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        model.eval()
        _, val_pred = model(val_x, val_y1, None, True)
        val_acc, val_p, val_r, val_f = get_acc_f1(val_labels, torch.argmax(val_pred, 1).detach().cpu().numpy().tolist())
        if val_f > best_val_f:
            best_val_f = val_f
            torch.save(model.state_dict(), DATA_PATH+'best_model.pt')  

    model = LabelMe_Classifier('stl').cuda()
    model.load_state_dict(torch.load(DATA_PATH+'best_model.pt'))
    model.eval()
    _, test_pred = model(test_x, test_y1, test_y2, True)
    test_pred = test_pred.detach().cpu().numpy()
    predicted_test_labels = np.argmax(test_pred, 1).tolist()

    ct_dictionary[str(exp)] = test_pred.tolist()
    true_labels_hard[str(exp)] = test_labels

    test_acc, test_p, test_r, test_f = get_acc_f1(test_labels, predicted_test_labels)
    cp, cr, cf = get_ct_f1(test_labels, predicted_test_labels, test_distrs)

    jsd, kl = get_jsd_kl_div(test_softs, test_pred)

    preds_ents = [entropy(p)/entropy(norm) for p in test_pred]

    ent = cosine_similarity(np.array(test_entropys).reshape(1, NUM_TEST), np.array(preds_ents).reshape(1, NUM_TEST))[0][0]

    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_pred, test_softs)
    
    accs.append(test_acc)
    prfs.append([test_p, test_r, test_f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(test_acc, test_f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

import json

writepath = writepath = 'drive/My Drive/Colab Notebooks/Significance_Testing/lm_experiments/'

name = 'ct'
with open(writepath+'lm_' + name + '.jsonlines', 'w') as f:
    json.dump(ct_dictionary, f)

with open(writepath+'lm_trueLabels.jsonlines', 'w') as f:
    json.dump(true_labels_hard, f)

print('CT_SRS Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nCT_SRS PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nCT_SRS Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nCT_SRS JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nCT_SRS KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nCT_SRSd entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nCT_SRS entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nCT_SRS crossentropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

