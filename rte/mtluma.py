# -*- coding: utf-8 -*-
"""sm_mtl_uma_rte.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dXi4tt57_Qown_QEZHfi5YfcexZEtN-P
"""

import csv
import numpy as np
import spacy
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial import distance
from scipy.special import kl_div
from scipy.stats import entropy
import torch
from tabulate import tabulate

torch.cuda.is_available()

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
sentence_encoder = SentenceTransformer('bert-base-nli-mean-tokens')

from google.colab import drive

drive.mount('/content/drive')

DATAPATH = 'drive/My Drive/Data/rte'

all_data = []

with open(DATAPATH+"/rte.standardized.tsv",encoding='utf8' ) as fd:
    rd = csv.reader(fd, delimiter="\t")
    for row in rd:
        if row != []:
            all_data.append(row)

all_responses = {}
gold_responses = {}

for annotation in all_data[1:]:
    task_id = annotation[2]
    annotator = annotation[1]
    response = annotation[-2]
    gold = annotation[-1]
    if task_id not in gold_responses:
        gold_responses[task_id] = int(gold)
        all_responses[task_id] = {annotator:int(response)}
    else:
        all_responses[task_id][annotator] = int(response)

dd = []
with open(DATAPATH+"/rte1.tsv",encoding='utf8' ) as fd:
    rd = csv.reader(fd, delimiter="\t")
    for row in rd:
        if row != []:
            dd.append(row)
            
data = {}
for id_, val, task, text, hyp in dd:
    data[id_] = [text, hyp]

from scipy.special import softmax

# creating the train and development data.
train_text = []
train_hypothesis = []
train_answers = []
train_gold = []
train_maj = []
train_distr = []
train_soft = []

for item, annotation in all_responses.items():
    crowd_labels = list(annotation.values())
    num_annotations = len(crowd_labels)
    train_answers.append(crowd_labels)
    train_text.append(data[item][0])
    train_hypothesis.append(data[item][1])
    train_gold.append(gold_responses[item])
    maj = max(crowd_labels,key=crowd_labels.count)
    train_maj.append(maj)
    distr = [crowd_labels.count(0), crowd_labels.count(1)]
    train_distr.append(distr)
    train_soft.append(softmax(distr))

embedded_text = sentence_encoder.encode(train_text)
embedded_hypothesis = sentence_encoder.encode(train_hypothesis)

embedded_text = sentence_encoder.encode(train_text)
embedded_hypothesis = sentence_encoder.encode(train_hypothesis)

norm = [0.5, 0.5]
item_entropys = [entropy(scores)/entropy(norm) for scores in train_soft]
item_entropys[0]

train_gold.count(0), train_gold.count(1), train_maj.count(0), train_maj.count(1)

def get_ct_f1(test_trues, test_preds, test_distrs, num_classes=2):
    total = 0
    correct = 0
    tp = {0:0, 1:0}
    fp = {0:0, 1:0}
    fn = {0:0, 1:0}
    gold = {0:0, 1:0}
    
    for p, g, distr in zip(test_preds, test_trues, test_distrs):
        if p > 0.5:
            p = 1
        else:
            p = 0
        # srs score of the gold not the predicted. Todo: talk it over with Massimo
        unit_vector = np.array([1 if i==g else 0 for i in range(num_classes)]).reshape(1,num_classes)
        distr = np.array(distr).reshape(1, num_classes)
        srs_s = cosine_similarity(unit_vector, distr)[0][0]
            
        if p == g:
            tp[p] += srs_s # correct hit
        else:
            fp[p] += (1-srs_s)  # miss
            fn[g] += srs_s   # correct rejection
    
        gold[g] += 1

    recall = {}
    precision = {}
    f1 = {}
    for i in range(num_classes):
        precision[i] = 1.0 * tp[i] / (tp[i] + fp[i]) if (fp[i] + tp[i]) != 0 else 0
        recall[i] = 1.0 * tp[i] / (tp[i] + fn[i]) if (fn[i] + tp[i]) !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0
        
    support = np.array([gold[i] for i in range(num_classes)])

    average_recall = np.average([recall[i] for i in range(num_classes)], weights=support)
    average_precision = np.average([precision[i] for i in range(num_classes)], weights=support)
    average_f1 = np.average([f1[i] for i in range(num_classes)], weights=support)

        
    return average_precision, average_recall, average_f1

def get_acc_f1(test_trues, test_preds):
    total = 0
    correct = 0
    matches = {0:0, 1:0}
    gold = {0:0, 1:0}
    system = {0:0, 1:0}

    for p, g in zip(test_preds,test_trues):
        total+=1
        if p > 0.5:
            p = 1
        else:
            p = 0
        if p == g:
            correct+=1
            matches[p] += 1

        gold[g] += 1
        system[p] += 1
    
    
    recall = {}
    precision = {}
    f1 = {}
    for i in range(2):
        recall[i] = 1.0 * matches[i] / gold[i] if matches[i] != 0 else 0
        precision[i] = 1.0 * matches[i] / system[i] if matches[i] !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0

    support = np.array([gold[0], gold[1]])

    average_recall = np.average([recall[i] for i in range(2)], weights=support)
    average_precision = np.average([precision[i] for i in range(2)], weights=support)
    average_f1 = np.average([f1[i] for i in range(2)], weights=support)
    
    acc = correct/total

    return acc, average_precision, average_recall, average_f1

def get_jsd_kl_div(soft_probs, predicted_probs):
    num_items = len(predicted_probs)
    all_jsd = [distance.jensenshannon(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    all_kl = [kl_div(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    return np.sum(all_jsd)/num_items, np.sum(all_kl)/num_items

def cross_entropy(predictions, targets, epsilon=1e-12):
    """
    Computes cross entropy between targets (encoded as one-hot vectors)
    and predictions.
    Input: predictions (N, k) ndarray
           targets (N, k) ndarray
    Returns: scalar
    """
    predictions = np.clip(predictions, epsilon, 1. - epsilon)
    N = predictions.shape[0]
    ce = -np.sum(targets*np.log(predictions))/N
    return ce

embedded_text = embedded_text.tolist()
embedded_hypothesis = embedded_hypothesis.tolist()

"""**MTL UMA**"""

class RTE_model(torch.nn.Module):
    def __init__(self, mtype='stl', smythe='kl'):
        super().__init__()

        # concat_size = embedded_hypothesis[0].shape[0]*4
        # final_size = embedded_hypothesis[0].shape[0]
        concat_size = 768 * 4
        final_size = 768
        self.fulcon = torch.nn.Sequential(torch.nn.Linear(concat_size, int(concat_size*0.8)),
                                           torch.nn.ReLU(),
                                           torch.nn.Linear(int(concat_size*0.8), int(concat_size*0.5)),
                                           torch.nn.ReLU(),
                                           torch.nn.Linear(int(concat_size*0.5), final_size),
                                          torch.nn.ReLU())
                
        self.output_hot = torch.nn.Linear(final_size, 2)
        self.output_soft = torch.nn.Linear(final_size, 1)
            

    def forward(self, text, hypothesis, one_hot_labels, soft_labels, weights, eval=False):
 
        concat_input = torch.cat([text, hypothesis, text*hypothesis, torch.abs(text-hypothesis)], 1)

        ful = self.fulcon(concat_input)
        main_predictions = self.output_hot(ful)
        softmax_scores = torch.softmax(main_predictions, 1)

        aux_predictions = self.output_soft(ful)
        item_diff = torch.sigmoid(aux_predictions).squeeze(1)
        if eval:
            return None, None, softmax_scores, item_diff
        else:
            one_hot_labels = torch.nn.functional.one_hot(one_hot_labels, 2)
            cross_entropy = torch.sum(torch.mul(one_hot_labels, softmax_scores.log()), 1) #* weights
            # print(cross_entropy.shape)
            main_loss = -torch.sum(cross_entropy)

            aux_loss = torch.nn.MSELoss(reduction='sum')(weights, item_diff)
            return main_loss, aux_loss, softmax_scores, item_diff

def observed_agreement(counted_votes):
    """Aggrement is computed using Observed Agreement instead of
    Kappa i.e. (Ao-Ae)/(1-Ae) as expected agreement is not computed
    on a per item basis. TODO confirm with Massimo.
    """
    c = sum(counted_votes)
    # getting the summ product
    numerator = sum([i*(i-1) for i in counted_votes])
    if c == 1:
        return 1.0  # if only one annotator annotated it, it is a perfect agreement
    return numerator/(c*(c-1))

item_weights = [observed_agreement(a) for a in train_distr]

test_diffs = np.array([observed_agreement(a) for a in train_distr])

NUM_EXPERIMENTS = 30
NUM_TEST = 800

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

similarity_diffs = []
diffs_correlation = []

mtl_oa_dictionary = {}

for exp in range(NUM_EXPERIMENTS):
    start = 0
    end = 80

    test_hots = []
    test_softs = []
    test_preds_hot = []
    test_preds_soft = []
    test_entropys = []
    test_distrs = []

    test_pred_diffs = []

    print('\nExperiment %d ###########'%exp)
    for i in range(10):
        print(i,'.....',)
        
        train_x1 = embedded_text[:start] + embedded_text[end:]
        train_x2 = embedded_hypothesis[:start] + embedded_hypothesis[end:]
        train_y1 = train_gold[:start] + train_gold[end:]
        train_y2 = train_soft[:start] + train_soft[end:]
        train_weights = item_weights[:start] + item_weights[end:]

        test_x1 = embedded_text[start:end]
        test_x2 = embedded_hypothesis[start:end]
        test_y1 = train_gold[start:end]
        test_y2 = train_soft[start:end]

        assert len(train_x1) == 720
        assert len(test_x1) == 80

        train_x1, train_x2, train_y1 = torch.tensor(train_x1).float().cuda(), torch.tensor(train_x2).float().cuda(), torch.tensor(train_y1).long().cuda()
        test_x1, test_x2, test_y1 = torch.tensor(test_x1).float().cuda(), torch.tensor(test_x2).float().cuda(), torch.tensor(test_y1).float().cuda()
        train_y2, test_y2 = torch.tensor(train_y2).float().cuda(), torch.tensor(test_y2).float().cuda()
        train_y3 = torch.tensor(train_weights).float().cuda()

        # load embeddings for that dataset
        model = RTE_model('mtl').cuda()
        optimizer = torch.optim.Adam(params=[p for p in model.parameters()
                                               if p.requires_grad],
                                       lr=0.0001)

        num_epochs = 20

        for epoch in range(1, num_epochs+1):

            model.train()
            hard_loss, soft_loss,_,_ = model(train_x1, train_x2, train_y1, train_y2, train_y3, False)
            # each backward step accumulates the gradients which are later backpropagated by step function
            optimizer.zero_grad()
            soft_loss.backward(retain_graph=True)
            hard_loss.backward()
            optimizer.step()
            
        model.eval()
        _, _,test_pred, pred_item_diffs = model(test_x1, test_x2, test_y1, test_y2, None, True)
        test_hots.extend(test_y1.detach().cpu().tolist())
        test_pred_h = torch.argmax(test_pred, 1)
        test_preds_hot.extend(test_pred_h.detach().cpu().tolist())
        test_preds_soft.extend(test_pred.detach().cpu().tolist())
        test_softs.extend(test_y2.detach().cpu().tolist())

        test_entropys.extend(item_entropys[start:end])
        test_distrs.extend(train_distr[start:end])

        test_pred_diffs.extend(pred_item_diffs.detach().cpu().tolist())

        start += 80
        end += 80
        
    mtl_oa_dictionary[str(exp)] = test_preds_soft

    acc, p, r, f = get_acc_f1(test_hots, test_preds_hot)
    cp, cr, cf = get_ct_f1(test_hots, test_preds_hot, test_distrs)
    jsd, kl = get_jsd_kl_div(test_softs, test_preds_soft)
    preds_ents = [entropy(p)/entropy(norm) for p in test_preds_soft]
    ent = cosine_similarity(np.array(test_entropys).reshape(1, 800), np.array(preds_ents).reshape(1, 800))[0][0]
    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_preds_soft, test_softs)

    
    itdiff = cosine_similarity(np.array(test_diffs).reshape(1, NUM_TEST), np.array(test_pred_diffs).reshape(1, NUM_TEST))[0][0]
    print('hree')
    itcorr = np.corrcoef(test_diffs, test_pred_diffs)[0][1]
    
    accs.append(acc)
    prfs.append([p, r, f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    similarity_diffs.append(itdiff)
    diffs_correlation.append(itcorr)

    print(acc, f, cf, jsd, kl, ent, corr, ce_res, itdiff, itcorr)
    print('#'*50)

import json

writepath = 'drive/My Drive/Colab Notebooks/Significance_Testing/rte_experiments/'

with open(writepath+'rte_mtl_oa.jsonlines', 'w') as f:
    json.dump(mtl_oa_dictionary, f)



print('MTL Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nMTL PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nMTL Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nMTL JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nMTL KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nMTL entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nMTL entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nMTL crossentropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))


print('\n\nchecking correlation with item difficulty')
print('\nMTL entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_diffs), np.max(similarity_diffs), np.min(similarity_diffs), np.std(similarity_diffs)))

print('\nMTL entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(diffs_correlation), np.max(diffs_correlation), np.min(diffs_correlation), np.std(diffs_correlation)))

np.array(test_diffs).reshape(1, NUM_TEST).shape

np.array(test_pred_diffs).squeeze(1).shape

np.array(test_diffs).shape

