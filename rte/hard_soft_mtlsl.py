# -*- coding: utf-8 -*-
"""corrected_softmax_all_rte_complete.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1chZicmpq6lUvwTGMp4KdwOqKFRpVif4T
"""

import csv
import numpy as np
import spacy
from sklearn.metrics.pairwise import cosine_similarity
from scipy.spatial import distance
from scipy.special import kl_div
from scipy.stats import entropy
import torch
from tabulate import tabulate

torch.cuda.is_available()

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
sentence_encoder = SentenceTransformer('bert-base-nli-mean-tokens')

from google.colab import drive

drive.mount('/content/drive')

DATAPATH = 'drive/My Drive/Data/rte'

all_data = []

with open(DATAPATH+"/rte.standardized.tsv",encoding='utf8' ) as fd:
    rd = csv.reader(fd, delimiter="\t")
    for row in rd:
        if row != []:
            all_data.append(row)

all_responses = {}
gold_responses = {}

for annotation in all_data[1:]:
    task_id = annotation[2]
    annotator = annotation[1]
    response = annotation[-2]
    gold = annotation[-1]
    if task_id not in gold_responses:
        gold_responses[task_id] = int(gold)
        all_responses[task_id] = {annotator:int(response)}
    else:
        all_responses[task_id][annotator] = int(response)

dd = []
with open(DATAPATH+"/rte1.tsv",encoding='utf8' ) as fd:
    rd = csv.reader(fd, delimiter="\t")
    for row in rd:
        if row != []:
            dd.append(row)
            
data = {}
for id_, val, task, text, hyp in dd:
    data[id_] = [text, hyp]

from scipy.special import softmax

# creating the train and development data.
train_text = []
train_hypothesis = []
train_answers = []
train_gold = []
train_maj = []
train_distr = []
train_soft = []

for item, annotation in all_responses.items():
    crowd_labels = list(annotation.values())
    num_annotations = len(crowd_labels)
    train_answers.append(crowd_labels)
    train_text.append(data[item][0])
    train_hypothesis.append(data[item][1])
    train_gold.append(gold_responses[item])
    maj = max(crowd_labels,key=crowd_labels.count)
    train_maj.append(maj)
    distr = [crowd_labels.count(0), crowd_labels.count(1)]
    train_distr.append(distr)
    train_soft.append(softmax(distr))

embedded_text = sentence_encoder.encode(train_text)
embedded_hypothesis = sentence_encoder.encode(train_hypothesis)

embedded_text = sentence_encoder.encode(train_text)
embedded_hypothesis = sentence_encoder.encode(train_hypothesis)

norm = [0.5, 0.5]
item_entropys = [entropy(scores)/entropy(norm) for scores in train_soft]
item_entropys[0]

train_gold.count(0), train_gold.count(1), train_maj.count(0), train_maj.count(1)

376/8, 424/8

47/50, 53/50

embedded_text.shape, embedded_hypothesis.shape

embedded_text[0:20].shape

"""**Checking my hypothesis about the order of soft eval**"""

filename = 'drive/My Drive/Data/rte/ds_posterior.npy'
train_ds_posterior = np.load(filename).tolist()
train_ds = np.argmax(train_ds_posterior, 1).tolist()

(np.array(train_maj) == np.array(train_ds)).sum() / 800

(np.array(train_maj) == np.array(train_gold)).sum() / 800

import json
with open(DATAPATH + '/mapped_preds.json', 'r') as fp:
    mace_data = json.load(fp)

mace_softs = [mace_data[item] for item, annotations in all_responses.items()]
mace_softs = np.array(mace_softs)
mace_labels = np.argmax(mace_softs, 1)

(np.array(train_maj) == mace_labels).sum() / len(mace_softs)



"""**Moving on...**"""

def get_ct_f1(test_trues, test_preds, test_distrs, num_classes=2):
    total = 0
    correct = 0
    tp = {0:0, 1:0}
    fp = {0:0, 1:0}
    fn = {0:0, 1:0}
    gold = {0:0, 1:0}
    
    for p, g, distr in zip(test_preds, test_trues, test_distrs):
        if p > 0.5:
            p = 1
        else:
            p = 0
        # srs score of the gold not the predicted. Todo: talk it over with Massimo
        unit_vector = np.array([1 if i==g else 0 for i in range(num_classes)]).reshape(1,num_classes)
        distr = np.array(distr).reshape(1, num_classes)
        srs_s = cosine_similarity(unit_vector, distr)[0][0]
            
        if p == g:
            tp[p] += srs_s # correct hit
        else:
            fp[p] += (1-srs_s)  # miss
            fn[g] += srs_s   # correct rejection
    
        gold[g] += 1

    recall = {}
    precision = {}
    f1 = {}
    for i in range(num_classes):
        precision[i] = 1.0 * tp[i] / (tp[i] + fp[i]) if (fp[i] + tp[i]) != 0 else 0
        recall[i] = 1.0 * tp[i] / (tp[i] + fn[i]) if (fn[i] + tp[i]) !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0
        
    support = np.array([gold[i] for i in range(num_classes)])

    average_recall = np.average([recall[i] for i in range(num_classes)], weights=support)
    average_precision = np.average([precision[i] for i in range(num_classes)], weights=support)
    average_f1 = np.average([f1[i] for i in range(num_classes)], weights=support)

        
    return average_precision, average_recall, average_f1

def get_acc_f1(test_trues, test_preds):
    total = 0
    correct = 0
    matches = {0:0, 1:0}
    gold = {0:0, 1:0}
    system = {0:0, 1:0}

    for p, g in zip(test_preds,test_trues):
        total+=1
        if p > 0.5:
            p = 1
        else:
            p = 0
        if p == g:
            correct+=1
            matches[p] += 1

        gold[g] += 1
        system[p] += 1
    
    
    recall = {}
    precision = {}
    f1 = {}
    for i in range(2):
        recall[i] = 1.0 * matches[i] / gold[i] if matches[i] != 0 else 0
        precision[i] = 1.0 * matches[i] / system[i] if matches[i] !=0 else 0
        f1[i] =  (2 * (precision[i] * recall[i])/(precision[i] + recall[i])) if (precision[i] + recall[i]) > 0 else 0

    support = np.array([gold[0], gold[1]])

    average_recall = np.average([recall[i] for i in range(2)], weights=support)
    average_precision = np.average([precision[i] for i in range(2)], weights=support)
    average_f1 = np.average([f1[i] for i in range(2)], weights=support)
    
    acc = correct/total

    return acc, average_precision, average_recall, average_f1

def get_jsd_kl_div(soft_probs, predicted_probs):
    num_items = len(predicted_probs)
    all_jsd = [distance.jensenshannon(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    all_kl = [kl_div(soft_probs[i], predicted_probs[i]) for i in range(num_items)]
    return np.sum(all_jsd)/num_items, np.sum(all_kl)/num_items

def cross_entropy(predictions, targets, epsilon=1e-12):
    """
    Computes cross entropy between targets (encoded as one-hot vectors)
    and predictions.
    Input: predictions (N, k) ndarray
           targets (N, k) ndarray
    Returns: scalar
    """
    predictions = np.clip(predictions, epsilon, 1. - epsilon)
    N = predictions.shape[0]
    ce = -np.sum(targets*np.log(predictions))/N
    return ce

class RTE_model(torch.nn.Module):
    def __init__(self, mtype='stl', softloss='kl'):
        super().__init__()

        # concat_size = embedded_hypothesis[0].shape[0]*4
        # final_size = embedded_hypothesis[0].shape[0]
        concat_size = 768 * 4
        final_size = 768
        self.fulcon = torch.nn.Sequential(torch.nn.Linear(concat_size, int(concat_size*0.8)),
                                           torch.nn.ReLU(),
                                           torch.nn.Linear(int(concat_size*0.8), int(concat_size*0.5)),
                                           torch.nn.ReLU(),
                                           torch.nn.Linear(int(concat_size*0.5), final_size),
                                          torch.nn.ReLU())
                
        if mtype == 'stl':
            self.output_hot = torch.nn.Linear(final_size, 1)
        elif mtype == 'softloss':
            self.output_soft = torch.nn.Linear(final_size, 2)
        elif mtype == 'mtl':
            self.output_hot = torch.nn.Linear(final_size, 1)
            self.output_soft = torch.nn.Linear(final_size, 2)
            
        self.mtype = mtype
        self.softloss = softloss
            

    def forward(self, text, hypothesis, one_hot_labels, soft_labels):
 
        concat_input = torch.cat([text, hypothesis, text*hypothesis, torch.abs(text-hypothesis)], 1)

        ful = self.fulcon(concat_input)

        if self.mtype == 'stl':
            pred_hard = self.output_soft(ful)
            hard_scores = torch.softmax(pred_hard, 1) 
            cross_entropy = torch.mul(one_hot_labels, hard_scores.log())
            hard_loss = -torch.sum(cross_entropy)
            return hard_loss, None, hard_scores
        elif self.mtype == 'softloss':
            pred_soft = self.output_soft(ful)
            softmax_scores = torch.softmax(pred_soft, 1) 
            if self.softloss == 'kl':
            #   soft_loss = torch.nn.KLDivLoss(reduction='mean')(soft_labels, softmax_scores)
                soft_loss = torch.sum(torch.mul(soft_labels, (torch.div(soft_labels, softmax_scores).log())))
            elif self.softloss == 'mse':
              soft_loss = torch.nn.MSELoss()(soft_labels, softmax_scores)
            elif self.softloss  == 'ce':
              cross_entropy = torch.mul(soft_labels, softmax_scores.log())
              soft_loss = -torch.sum(cross_entropy)
            return soft_loss, None, softmax_scores
        elif self.mtype == 'mtl':
            pred_hard = self.output_soft(ful)
            hard_scores = torch.softmax(pred_hard, 1) 
            cross_entropy = torch.mul(one_hot_labels, hard_scores.log())
            hard_loss = -torch.sum(cross_entropy)

            pred_soft = self.output_soft(ful)
            soft_scores = torch.softmax(pred_soft, 1) 
            soft_loss = torch.sum(torch.mul(soft_scores, (torch.div(soft_scores, soft_labels).log()))) 
            return hard_loss, soft_loss, hard_scores, soft_scores

embedded_text = embedded_text.tolist()
embedded_hypothesis = embedded_hypothesis.tolist()

writepath = 'drive/My Drive/Colab Notebooks/Significance_Testing/rte_experiments/'

"""**Training using gold, first getting gold_onehot**"""

hot_gold = [[1, 0] if item==0 else [0, 1] for item in train_gold]

(np.argmax(hot_gold, 1) == train_gold).sum() / 800

# Learning from the posterior and the CE divergence loss function

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []


gold_dictionary = {}
# true_labels_hard = {}
# true_labels_soft = {}

for exp in range(NUM_EXPERIMENTS):
    start = 0
    end = 80

    test_hots = []
    test_softs = []
    test_preds_hot = []
    test_preds_soft = []
    test_entropys = []
    test_distrs = []

    print('\nExperiment %d ###########'%exp)
    for i in range(10):
        print(i,'.....',)
        
        train_x1 = embedded_text[:start] + embedded_text[end:]
        train_x2 = embedded_hypothesis[:start] + embedded_hypothesis[end:]
        train_y1 = hot_gold[:start] + hot_gold[end:]
        train_y2 = hot_gold[:start] + hot_gold[end:]

        test_x1 = embedded_text[start:end]
        test_x2 = embedded_hypothesis[start:end]
        test_y1 = train_gold[start:end]
        test_y2 = train_soft[start:end]

        assert len(train_x1) == 720
        assert len(test_x1) == 80

        train_x1, train_x2, train_y1 = torch.tensor(train_x1).float().cuda(), torch.tensor(train_x2).float().cuda(), torch.tensor(train_y1).float().cuda()
        test_x1, test_x2, test_y1 = torch.tensor(test_x1).float().cuda(), torch.tensor(test_x2).float().cuda(), torch.tensor(test_y1).float().cuda()
        train_y2, test_y2 = torch.tensor(train_y2).float().cuda(), torch.tensor(test_y2).float().cuda()

        # load embeddings for that dataset
        model = RTE_model('softloss', 'ce').cuda()
        optimizer = torch.optim.Adam(params=[p for p in model.parameters()
                                               if p.requires_grad],
                                       lr=0.0001)

        num_epochs = 20

        for epoch in range(1, num_epochs+1):

            model.train()
            loss, _,_ = model(train_x1, train_x2, train_y1, train_y2)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        model.eval()
        _, _,test_pred = model(test_x1, test_x2, test_y1, test_y2)
        test_hots.extend(test_y1.detach().cpu().tolist())
        test_preds_hot.extend(torch.argmax(test_pred, 1).detach().cpu().tolist())
        test_preds_soft.extend(test_pred.detach().cpu().tolist())
        test_softs.extend(test_y2.detach().cpu().tolist())
        test_entropys.extend(item_entropys[start:end])
        test_distrs.extend(train_distr[start:end])

        start += 80
        end += 80
        
    acc, p, r, f = get_acc_f1(test_hots, test_preds_hot)
    cp, cr, cf = get_ct_f1(test_hots, test_preds_hot, test_distrs)
    jsd, kl = get_jsd_kl_div(test_softs, test_preds_soft)
    preds_ents = [entropy(p)/entropy(norm) for p in test_preds_soft]
    ent = cosine_similarity(np.array(test_entropys).reshape(1, 800), np.array(preds_ents).reshape(1, 800))[0][0]
    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_preds_soft, test_softs)

    # true_labels_hard[exp] = test_hots
    # true_labels_soft[exp] = test_softs
    gold_dictionary[exp] = test_preds_soft
    
    accs.append(acc)
    prfs.append([p, r, f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(acc, f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

# with open(writepath+'rte_trueLabels.jsonlines', 'w') as f:
#     json.dump(true_labels_hard, f)

# with open(writepath+'rte_trueSofts.jsonlines', 'w') as f:
#     json.dump(true_labels_soft, f)

with open(writepath+'rte_gold.jsonlines', 'w') as f:
    json.dump(gold_dictionary, f)


print('Gold Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nGold PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nGold Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nGold JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nGold KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nGold entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nGold entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nGold cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

"""**Training using MV but first getting hot mv**"""

# testing a theory
hot_mv = [[1, 0] if item==0 else [0, 1] for item in train_maj]

(np.argmax(hot_mv, 1) == train_maj).sum() / 800

# Learning from the posterior and the CE divergence loss function

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

mv_dictionary = {}

for exp in range(NUM_EXPERIMENTS):
    start = 0
    end = 80

    test_hots = []
    test_softs = []
    test_preds_hot = []
    test_preds_soft = []
    test_entropys = []
    test_distrs = []

    print('\nExperiment %d ###########'%exp)
    for i in range(10):
        print(i,'.....',)
        
        train_x1 = embedded_text[:start] + embedded_text[end:]
        train_x2 = embedded_hypothesis[:start] + embedded_hypothesis[end:]
        train_y1 = hot_mv[:start] + hot_mv[end:]
        train_y2 = hot_mv[:start] + hot_mv[end:]

        test_x1 = embedded_text[start:end]
        test_x2 = embedded_hypothesis[start:end]
        test_y1 = train_gold[start:end]
        test_y2 = train_soft[start:end]

        assert len(train_x1) == 720
        assert len(test_x1) == 80

        train_x1, train_x2, train_y1 = torch.tensor(train_x1).float().cuda(), torch.tensor(train_x2).float().cuda(), torch.tensor(train_y1).float().cuda()
        test_x1, test_x2, test_y1 = torch.tensor(test_x1).float().cuda(), torch.tensor(test_x2).float().cuda(), torch.tensor(test_y1).float().cuda()
        train_y2, test_y2 = torch.tensor(train_y2).float().cuda(), torch.tensor(test_y2).float().cuda()

        # load embeddings for that dataset
        model = RTE_model('softloss', 'ce').cuda()
        optimizer = torch.optim.Adam(params=[p for p in model.parameters()
                                               if p.requires_grad],
                                       lr=0.0001)

        num_epochs = 20

        for epoch in range(1, num_epochs+1):

            model.train()
            loss, _,_ = model(train_x1, train_x2, train_y1, train_y2)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        model.eval()
        _, _,test_pred = model(test_x1, test_x2, test_y1, test_y2)
        test_hots.extend(test_y1.detach().cpu().tolist())
        test_preds_hot.extend(torch.argmax(test_pred, 1).detach().cpu().tolist())
        test_preds_soft.extend(test_pred.detach().cpu().tolist())
        test_softs.extend(test_y2.detach().cpu().tolist())
        test_entropys.extend(item_entropys[start:end])
        test_distrs.extend(train_distr[start:end])

        start += 80
        end += 80
        
    acc, p, r, f = get_acc_f1(test_hots, test_preds_hot)
    cp, cr, cf = get_ct_f1(test_hots, test_preds_hot, test_distrs)
    jsd, kl = get_jsd_kl_div(test_softs, test_preds_soft)
    preds_ents = [entropy(p)/entropy(norm) for p in test_preds_soft]
    ent = cosine_similarity(np.array(test_entropys).reshape(1, 800), np.array(preds_ents).reshape(1, 800))[0][0]
    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_preds_soft, test_softs)

    mv_dictionary[exp] = test_preds_soft

    
    accs.append(acc)
    prfs.append([p, r, f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(acc, f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

print('MV Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nMV PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nMV Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nMV JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nMV KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nMV entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nMV entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nMV cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

with open(writepath+'rte_mv.jsonlines', 'w') as f:
    json.dump(mv_dictionary, f)


print('MV Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nMV PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nMV Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nMV JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nMV KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nMV entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nMV entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nMV cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

"""**Training using MACE but first getting mace hot**"""

# testing a theory
hot_mace = [[1, 0] if item==0 else [0, 1] for item in mace_labels]

(np.argmax(hot_mace, 1) == mace_labels).sum() / 800

# Learning from the posterior and the CE divergence loss function

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

mace_dictionary = {}

for exp in range(NUM_EXPERIMENTS):
    start = 0
    end = 80

    test_hots = []
    test_softs = []
    test_preds_hot = []
    test_preds_soft = []
    test_entropys = []
    test_distrs = []

    print('\nExperiment %d ###########'%exp)
    for i in range(10):
        print(i,'.....',)
        
        train_x1 = embedded_text[:start] + embedded_text[end:]
        train_x2 = embedded_hypothesis[:start] + embedded_hypothesis[end:]
        train_y1 = hot_mace[:start] + hot_mace[end:]
        train_y2 = hot_mace[:start] + hot_mace[end:]

        test_x1 = embedded_text[start:end]
        test_x2 = embedded_hypothesis[start:end]
        test_y1 = train_gold[start:end]
        test_y2 = train_soft[start:end]

        assert len(train_x1) == 720
        assert len(test_x1) == 80

        train_x1, train_x2, train_y1 = torch.tensor(train_x1).float().cuda(), torch.tensor(train_x2).float().cuda(), torch.tensor(train_y1).float().cuda()
        test_x1, test_x2, test_y1 = torch.tensor(test_x1).float().cuda(), torch.tensor(test_x2).float().cuda(), torch.tensor(test_y1).float().cuda()
        train_y2, test_y2 = torch.tensor(train_y2).float().cuda(), torch.tensor(test_y2).float().cuda()

        # load embeddings for that dataset
        model = RTE_model('softloss', 'ce').cuda()
        optimizer = torch.optim.Adam(params=[p for p in model.parameters()
                                               if p.requires_grad],
                                       lr=0.0001)

        num_epochs = 20

        for epoch in range(1, num_epochs+1):

            model.train()
            loss, _,_ = model(train_x1, train_x2, train_y1, train_y2)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        model.eval()
        _, _,test_pred = model(test_x1, test_x2, test_y1, test_y2)
        test_hots.extend(test_y1.detach().cpu().tolist())
        test_preds_hot.extend(torch.argmax(test_pred, 1).detach().cpu().tolist())
        test_preds_soft.extend(test_pred.detach().cpu().tolist())
        test_softs.extend(test_y2.detach().cpu().tolist())
        test_entropys.extend(item_entropys[start:end])
        test_distrs.extend(train_distr[start:end])

        start += 80
        end += 80
        
    acc, p, r, f = get_acc_f1(test_hots, test_preds_hot)
    cp, cr, cf = get_ct_f1(test_hots, test_preds_hot, test_distrs)
    jsd, kl = get_jsd_kl_div(test_softs, test_preds_soft)
    preds_ents = [entropy(p)/entropy(norm) for p in test_preds_soft]
    ent = cosine_similarity(np.array(test_entropys).reshape(1, 800), np.array(preds_ents).reshape(1, 800))[0][0]
    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_preds_soft, test_softs)

    mace_dictionary[exp] = test_preds_soft

    
    accs.append(acc)
    prfs.append([p, r, f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(acc, f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

name = 'mace'
with open(writepath+'rte_' + name + '.jsonlines', 'w') as f:
    json.dump(mace_dictionary, f)


print('Mace Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nMace PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nMace Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nMace JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nMace KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nMace entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nMace entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nMace cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

"""**Training using DS but first getting mace hot**"""

# testing a theory
hot_ds = [[1, 0] if item==0 else [0, 1] for item in train_ds]

(np.argmax(hot_ds, 1) == train_ds).sum() / 800

# Learning from the posterior and the CE divergence loss function

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

ds_dictionary = {}

for exp in range(NUM_EXPERIMENTS):
    start = 0
    end = 80

    test_hots = []
    test_softs = []
    test_preds_hot = []
    test_preds_soft = []
    test_entropys = []
    test_distrs = []

    print('\nExperiment %d ###########'%exp)
    for i in range(10):
        print(i,'.....',)
        
        train_x1 = embedded_text[:start] + embedded_text[end:]
        train_x2 = embedded_hypothesis[:start] + embedded_hypothesis[end:]
        train_y1 = hot_ds[:start] + hot_ds[end:]
        train_y2 = hot_ds[:start] + hot_ds[end:]

        test_x1 = embedded_text[start:end]
        test_x2 = embedded_hypothesis[start:end]
        test_y1 = train_gold[start:end]
        test_y2 = train_soft[start:end]

        assert len(train_x1) == 720
        assert len(test_x1) == 80

        train_x1, train_x2, train_y1 = torch.tensor(train_x1).float().cuda(), torch.tensor(train_x2).float().cuda(), torch.tensor(train_y1).float().cuda()
        test_x1, test_x2, test_y1 = torch.tensor(test_x1).float().cuda(), torch.tensor(test_x2).float().cuda(), torch.tensor(test_y1).float().cuda()
        train_y2, test_y2 = torch.tensor(train_y2).float().cuda(), torch.tensor(test_y2).float().cuda()

        # load embeddings for that dataset
        model = RTE_model('softloss', 'ce').cuda()
        optimizer = torch.optim.Adam(params=[p for p in model.parameters()
                                               if p.requires_grad],
                                       lr=0.0001)

        num_epochs = 20

        for epoch in range(1, num_epochs+1):

            model.train()
            loss, _,_ = model(train_x1, train_x2, train_y1, train_y2)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        model.eval()
        _, _,test_pred = model(test_x1, test_x2, test_y1, test_y2)
        test_hots.extend(test_y1.detach().cpu().tolist())
        test_preds_hot.extend(torch.argmax(test_pred, 1).detach().cpu().tolist())
        test_preds_soft.extend(test_pred.detach().cpu().tolist())
        test_softs.extend(test_y2.detach().cpu().tolist())
        test_entropys.extend(item_entropys[start:end])
        test_distrs.extend(train_distr[start:end])

        start += 80
        end += 80
        
    acc, p, r, f = get_acc_f1(test_hots, test_preds_hot)
    cp, cr, cf = get_ct_f1(test_hots, test_preds_hot, test_distrs)
    jsd, kl = get_jsd_kl_div(test_softs, test_preds_soft)
    preds_ents = [entropy(p)/entropy(norm) for p in test_preds_soft]
    ent = cosine_similarity(np.array(test_entropys).reshape(1, 800), np.array(preds_ents).reshape(1, 800))[0][0]
    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_preds_soft, test_softs)

    ds_dictionary[exp] = test_preds_soft

    
    accs.append(acc)
    prfs.append([p, r, f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(acc, f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

name = 'ds'
with open(writepath+'rte_' + name + '.jsonlines', 'w') as f:
    json.dump(ds_dictionary, f)


print(name.upper() + ' Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\n' + name.upper() + ' Mace PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\n'  + name.upper() + ' Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\n' + name.upper() + ' JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\n' + name.upper() + ' KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\n' + name.upper() + ' entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\n' + name.upper() + ' entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\n' + name.upper() + ' cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

# Learning from the posterior and the KL divergence loss function
NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

kl_dictionary = {}

for exp in range(NUM_EXPERIMENTS):
    start = 0
    end = 80

    test_hots = []
    test_softs = []
    test_preds_hot = []
    test_preds_soft = []
    test_entropys = []
    test_distrs = []

    print('\nExperiment %d ###########'%exp)
    for i in range(10):
        print(i,'.....',)
        
        train_x1 = embedded_text[:start] + embedded_text[end:]
        train_x2 = embedded_hypothesis[:start] + embedded_hypothesis[end:]
        train_y1 = train_gold[:start] + train_gold[end:]
        train_y2 = train_soft[:start] + train_soft[end:]

        test_x1 = embedded_text[start:end]
        test_x2 = embedded_hypothesis[start:end]
        test_y1 = train_gold[start:end]
        test_y2 = train_soft[start:end]

        assert len(train_x1) == 720
        assert len(test_x1) == 80

        train_x1, train_x2, train_y1 = torch.tensor(train_x1).float().cuda(), torch.tensor(train_x2).float().cuda(), torch.tensor(train_y1).float().cuda()
        test_x1, test_x2, test_y1 = torch.tensor(test_x1).float().cuda(), torch.tensor(test_x2).float().cuda(), torch.tensor(test_y1).float().cuda()
        train_y2, test_y2 = torch.tensor(train_y2).float().cuda(), torch.tensor(test_y2).float().cuda()

        # load embeddings for that dataset
        model = RTE_model('softloss', 'kl').cuda()
        optimizer = torch.optim.Adam(params=[p for p in model.parameters()
                                               if p.requires_grad],
                                       lr=0.0001)

        num_epochs = 20

        for epoch in range(1, num_epochs+1):

            model.train()
            loss, _,_ = model(train_x1, train_x2, train_y1, train_y2)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        model.eval()
        _, _,test_pred = model(test_x1, test_x2, test_y1, test_y2)
        test_hots.extend(test_y1.detach().cpu().tolist())
        test_preds_hot.extend(torch.argmax(test_pred, 1).detach().cpu().tolist())
        test_preds_soft.extend(test_pred.detach().cpu().tolist())
        test_softs.extend(test_y2.detach().cpu().tolist())
        test_entropys.extend(item_entropys[start:end])
        test_distrs.extend(train_distr[start:end])

        start += 80
        end += 80
        
    kl_dictionary[exp] = test_preds_soft

    acc, p, r, f = get_acc_f1(test_hots, test_preds_hot)
    cp, cr, cf = get_ct_f1(test_hots, test_preds_hot, test_distrs)
    jsd, kl = get_jsd_kl_div(test_softs, test_preds_soft)
    preds_ents = [entropy(p)/entropy(norm) for p in test_preds_soft]
    ent = cosine_similarity(np.array(test_entropys).reshape(1, 800), np.array(preds_ents).reshape(1, 800))[0][0]
    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_preds_soft, test_softs)
    
    accs.append(acc)
    prfs.append([p, r, f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(acc, f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

with open(writepath+'rte_kl.jsonlines', 'w') as f:
    json.dump(kl_dictionary, f)


print('Softloss_KL Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nSoftloss_KL PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nSoftloss_KL Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nSoftloss_KL JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nSoftloss_KL KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nSoftloss_KL entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nSoftloss_KL entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nSoftloss_KL cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

# Learning from the posterior and the MSE divergence loss function
NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

mse_dictionary = {}

for exp in range(NUM_EXPERIMENTS):
    start = 0
    end = 80

    test_hots = []
    test_softs = []
    test_preds_hot = []
    test_preds_soft = []
    test_entropys = []
    test_distrs = []

    print('\nExperiment %d ###########'%exp)
    for i in range(10):
        print(i,'.....',)
        
        train_x1 = embedded_text[:start] + embedded_text[end:]
        train_x2 = embedded_hypothesis[:start] + embedded_hypothesis[end:]
        train_y1 = train_gold[:start] + train_gold[end:]
        train_y2 = train_soft[:start] + train_soft[end:]

        test_x1 = embedded_text[start:end]
        test_x2 = embedded_hypothesis[start:end]
        test_y1 = train_gold[start:end]
        test_y2 = train_soft[start:end]

        assert len(train_x1) == 720
        assert len(test_x1) == 80

        train_x1, train_x2, train_y1 = torch.tensor(train_x1).float().cuda(), torch.tensor(train_x2).float().cuda(), torch.tensor(train_y1).float().cuda()
        test_x1, test_x2, test_y1 = torch.tensor(test_x1).float().cuda(), torch.tensor(test_x2).float().cuda(), torch.tensor(test_y1).float().cuda()
        train_y2, test_y2 = torch.tensor(train_y2).float().cuda(), torch.tensor(test_y2).float().cuda()

        # load embeddings for that dataset
        model = RTE_model('softloss', 'mse').cuda()
        optimizer = torch.optim.Adam(params=[p for p in model.parameters()
                                               if p.requires_grad],
                                       lr=0.0001)

        num_epochs = 20

        for epoch in range(1, num_epochs+1):

            model.train()
            loss, _,_ = model(train_x1, train_x2, train_y1, train_y2)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        model.eval()
        _, _,test_pred = model(test_x1, test_x2, test_y1, test_y2)
        test_hots.extend(test_y1.detach().cpu().tolist())
        test_preds_hot.extend(torch.argmax(test_pred, 1).detach().cpu().tolist())
        test_preds_soft.extend(test_pred.detach().cpu().tolist())
        test_softs.extend(test_y2.detach().cpu().tolist())
        test_entropys.extend(item_entropys[start:end])
        test_distrs.extend(train_distr[start:end])

        start += 80
        end += 80

    mse_dictionary[exp] = test_preds_soft
        
    acc, p, r, f = get_acc_f1(test_hots, test_preds_hot)
    cp, cr, cf = get_ct_f1(test_hots, test_preds_hot, test_distrs)
    jsd, kl = get_jsd_kl_div(test_softs, test_preds_soft)
    preds_ents = [entropy(p)/entropy(norm) for p in test_preds_soft]
    ent = cosine_similarity(np.array(test_entropys).reshape(1, 800), np.array(preds_ents).reshape(1, 800))[0][0]
    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_preds_soft, test_softs)
    
    accs.append(acc)
    prfs.append([p, r, f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(acc, f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

with open(writepath+'rte_mse.jsonlines', 'w') as f:
    json.dump(mse_dictionary, f)


print('Softloss_MSE Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nSoftloss_MSE PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nSoftloss_MSE Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nSoftloss_MSE JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nSoftloss_MSE KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nSoftloss_MSE entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nSoftloss_MSE entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nSoftloss_MSE cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

# Learning from the posterior and the CE divergence loss function

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []

ce_dictionary = {}

for exp in range(NUM_EXPERIMENTS):
    start = 0
    end = 80

    test_hots = []
    test_softs = []
    test_preds_hot = []
    test_preds_soft = []
    test_entropys = []
    test_distrs = []

    print('\nExperiment %d ###########'%exp)
    for i in range(10):
        print(i,'.....',)
        
        train_x1 = embedded_text[:start] + embedded_text[end:]
        train_x2 = embedded_hypothesis[:start] + embedded_hypothesis[end:]
        train_y1 = train_gold[:start] + train_gold[end:]
        train_y2 = train_soft[:start] + train_soft[end:]

        test_x1 = embedded_text[start:end]
        test_x2 = embedded_hypothesis[start:end]
        test_y1 = train_gold[start:end]
        test_y2 = train_soft[start:end]

        assert len(train_x1) == 720
        assert len(test_x1) == 80

        train_x1, train_x2, train_y1 = torch.tensor(train_x1).float().cuda(), torch.tensor(train_x2).float().cuda(), torch.tensor(train_y1).float().cuda()
        test_x1, test_x2, test_y1 = torch.tensor(test_x1).float().cuda(), torch.tensor(test_x2).float().cuda(), torch.tensor(test_y1).float().cuda()
        train_y2, test_y2 = torch.tensor(train_y2).float().cuda(), torch.tensor(test_y2).float().cuda()

        # load embeddings for that dataset
        model = RTE_model('softloss', 'ce').cuda()
        optimizer = torch.optim.Adam(params=[p for p in model.parameters()
                                               if p.requires_grad],
                                       lr=0.0001)

        num_epochs = 20

        for epoch in range(1, num_epochs+1):

            model.train()
            loss, _,_ = model(train_x1, train_x2, train_y1, train_y2)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        model.eval()
        _, _,test_pred = model(test_x1, test_x2, test_y1, test_y2)
        test_hots.extend(test_y1.detach().cpu().tolist())
        test_preds_hot.extend(torch.argmax(test_pred, 1).detach().cpu().tolist())
        test_preds_soft.extend(test_pred.detach().cpu().tolist())
        test_softs.extend(test_y2.detach().cpu().tolist())
        test_entropys.extend(item_entropys[start:end])
        test_distrs.extend(train_distr[start:end])

        start += 80
        end += 80
        
    acc, p, r, f = get_acc_f1(test_hots, test_preds_hot)
    cp, cr, cf = get_ct_f1(test_hots, test_preds_hot, test_distrs)
    jsd, kl = get_jsd_kl_div(test_softs, test_preds_soft)
    preds_ents = [entropy(p)/entropy(norm) for p in test_preds_soft]
    ent = cosine_similarity(np.array(test_entropys).reshape(1, 800), np.array(preds_ents).reshape(1, 800))[0][0]
    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_preds_soft, test_softs)

    ce_dictionary[exp] = test_preds_soft

    
    accs.append(acc)
    prfs.append([p, r, f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(acc, f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

with open(writepath+'rte_ce.jsonlines', 'w') as f:
    json.dump(ce_dictionary, f)


print('Softloss_CE Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nSoftloss_CE PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nSoftloss_CE Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nSoftloss_CE JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nSoftloss_CE KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nSoftloss_CE entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nSoftloss_CE entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nSoftloss_CE cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

"""**Training using mtl, the proper way, with inverse kl and everything**"""

# Learning using MTL, i.e. jointly learning the posterior and the gold label

NUM_EXPERIMENTS = 30

accs = []
prfs = []
ct_prfs = []
jsds = []
kls = []
similarity_ents = []
ents_correlation = []
ce_results = []


mtlhard_dictionary = {}
mtlsoft_dictionary = {}


for exp in range(NUM_EXPERIMENTS):
    start = 0
    end = 80

    test_hots = []
    test_softs = []
    test_preds_hot = []
    test_preds_soft = []
    test_entropys = []
    test_distrs = []

    print('\nExperiment %d ###########'%exp)
    for i in range(10):
        print(i,'.....',)
        
        train_x1 = embedded_text[:start] + embedded_text[end:]
        train_x2 = embedded_hypothesis[:start] + embedded_hypothesis[end:]
        train_y1 = hot_gold[:start] + hot_gold[end:]
        train_y2 = train_soft[:start] + train_soft[end:]

        test_x1 = embedded_text[start:end]
        test_x2 = embedded_hypothesis[start:end]
        test_y1 = hot_gold[start:end]
        test_y2 = train_soft[start:end]

        assert len(train_x1) == 720
        assert len(test_x1) == 80

        train_x1, train_x2, train_y1 = torch.tensor(train_x1).float().cuda(), torch.tensor(train_x2).float().cuda(), torch.tensor(train_y1).float().cuda()
        test_x1, test_x2, test_y1 = torch.tensor(test_x1).float().cuda(), torch.tensor(test_x2).float().cuda(), torch.tensor(test_y1).float().cuda()
        train_y2, test_y2 = torch.tensor(train_y2).float().cuda(), torch.tensor(test_y2).float().cuda()

        # load embeddings for that dataset
        model = RTE_model('mtl').cuda()
        optimizer = torch.optim.Adam(params=[p for p in model.parameters()
                                               if p.requires_grad],
                                       lr=0.0001)

        num_epochs = 20

        for epoch in range(1, num_epochs+1):

            model.train()
            hard_loss, soft_loss,_,_ = model(train_x1, train_x2, train_y1, train_y2)
            # each backward step accumulates the gradients which are later backpropagated by step function
            optimizer.zero_grad()
            soft_loss.backward(retain_graph=True)
            hard_loss.backward()
            optimizer.step()
            
        model.eval()
        _, _,test_pred_h, test_pred_s = model(test_x1, test_x2, test_y1, test_y2)
        test_hots.extend(torch.argmax(test_y1, 1).detach().cpu().tolist())
        test_preds_hot.extend(torch.argmax(test_pred_h, 1).detach().cpu().tolist())
        test_preds_soft.extend(test_pred_s.detach().cpu().tolist())
        test_softs.extend(test_y2.detach().cpu().tolist())
        test_entropys.extend(item_entropys[start:end])
        test_distrs.extend(train_distr[start:end])

        start += 80
        end += 80
        
    acc, p, r, f = get_acc_f1(test_hots, test_preds_hot)
    cp, cr, cf = get_ct_f1(test_hots, test_preds_hot, test_distrs)
    jsd, kl = get_jsd_kl_div(test_softs, test_preds_soft)
    preds_ents = [entropy(p)/entropy(norm) for p in test_preds_soft]
    ent = cosine_similarity(np.array(test_entropys).reshape(1, 800), np.array(preds_ents).reshape(1, 800))[0][0]
    corr = np.corrcoef(test_entropys, preds_ents)[0][1]
    ce_res = cross_entropy(test_preds_soft, test_softs)


    mtlhard_dictionary[exp] = test_preds_hot
    mtlsoft_dictionary[exp] = test_preds_soft
    
    accs.append(acc)
    prfs.append([p, r, f])
    ct_prfs.append([cp, cr, cf])
    jsds.append(jsd)
    kls.append(kl)
    similarity_ents.append(ent)
    ents_correlation.append(corr)
    ce_results.append(ce_res)
    
    print(acc, f, cf, jsd, kl, ent, corr, ce_res)
    print('#'*50)

print('USING INVERSE KL')

with open(writepath+'rte_mtlhard.jsonlines', 'w') as f:
    json.dump(mtlhard_dictionary, f)


with open(writepath+'rte_mtlsoft.jsonlines', 'w') as f:
    json.dump(mtlsoft_dictionary, f)


print('MTL Accuracy stats after 30 epochs: Avg %0.2f, Max %0.2f, Min %0.2f, Std %0.2f' %(np.average(accs)*100, np.max(accs)*100, np.min(accs)*100, np.std(accs)*100))

print('\nMTL PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(prfs, 0).tolist()
maxs = ['maximums'] + np.max(prfs, 0).tolist()
mins = ['minimums'] + np.min(prfs, 0).tolist()
stds = ['stds'] + np.std(prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['Precision', 'Recall', 'F1']))

print('\nMTL Crowdtruth PRF stats after 30 epochs:... ')
avgs = ['averages'] + np.average(ct_prfs, 0).tolist()
maxs = ['maximums'] + np.max(ct_prfs, 0).tolist()
mins = ['minimums'] + np.min(ct_prfs, 0).tolist()
stds = ['stds'] + np.std(ct_prfs, 0).tolist()
print(tabulate([avgs, maxs, mins, stds], headers=['CT Precision', 'CT Recall', 'CT F1']))

print('\nMTL JSD stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(jsds), np.max(jsds), np.min(jsds), np.std(jsds)))

print('\nMTL KL stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(kls), np.max(kls), np.min(kls), np.std(kls)))

print('\nMTL entropy similarity stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(similarity_ents), np.max(similarity_ents), np.min(similarity_ents), np.std(similarity_ents)))

print('\nMTL entropy correlation stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ents_correlation), np.max(ents_correlation), np.min(ents_correlation), np.std(ents_correlation)))

print('\nMTL cross entropy stats after 30 epochs: Avg %0.4f, Max %0.4f, Min %0.4f, Std %0.4f' %(np.average(ce_results), np.max(ce_results), np.min(ce_results), np.std(ce_results)))

